{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AAA-ped-w7-c1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MostaSchoolOfAI/All-About-AI-Python-Edition/blob/master/week%207-Recommender%20Systems/AAA_ped_w7_c1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "b9q-VxEW5tRs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://docs.google.com/uc?export=download&id=1EiHSYfHYk8nKMEWd6A74CMFVak5Lf4ab\">\n",
        "# Recommender System: Neighborhood-Based Filtering"
      ]
    },
    {
      "metadata": {
        "id": "H1a7Uuzz8dnI",
        "colab_type": "toc"
      },
      "cell_type": "markdown",
      "source": [
        ">[Recommender System: Dimensionality Reduction](#scrollTo=b9q-VxEW5tRs)\n",
        "\n",
        ">[1-Introduction](#scrollTo=1oBho425Q7ca)\n",
        "\n",
        ">>[Categories](#scrollTo=A8DvRRWlRdGi)\n",
        "\n",
        ">>[Libraries](#scrollTo=OjzIaOe1RdZ2)\n",
        "\n",
        ">[2- Collaborative filtering](#scrollTo=VL49qQMiRCfC)\n",
        "\n",
        ">>[Neighborhood approach](#scrollTo=QtI6YjSJRzo8)\n",
        "\n",
        ">>[Model based approach](#scrollTo=Tg2exEipRz7w)\n",
        "\n",
        ">>[Knn algorithm](#scrollTo=Zd06of0ZR7JC)\n",
        "\n",
        ">[3- Similarity scores](#scrollTo=Nhm8OJc_RFU8)\n",
        "\n",
        ">>[Introduction](#scrollTo=TOgMnI_CR7QE)\n",
        "\n",
        ">>[Cosine Similarity](#scrollTo=iJCObGCDR7Wk)\n",
        "\n",
        ">>[Pearson similarity](#scrollTo=qFh5dIXNRbde)\n",
        "\n",
        ">[4- Cross-Validation](#scrollTo=Ca8Ofrs7RHwC)\n",
        "\n",
        ">>[Introduction](#scrollTo=xn8bZSmqSBwY)\n",
        "\n",
        ">>[K-fold Cross validation](#scrollTo=157T7zEfSB_u)\n",
        "\n",
        ">>[Cross-validation implementation](#scrollTo=ArfuFPFFSB4a)\n",
        "\n",
        ">[5- User-based Collaborative Filtering With Surprise](#scrollTo=5aXVjrUrRNGi)\n",
        "\n",
        ">>[Library and data](#scrollTo=H71qigLHSGU2)\n",
        "\n",
        ">>[Similarity matrix computation (fitting)](#scrollTo=yMgLDZAdSGjG)\n",
        "\n",
        ">>[Prediction](#scrollTo=Z7prBx0pSGbS)\n",
        "\n",
        ">[6- Item-based Collaborative Filtering With Surprise](#scrollTo=CffbTqAERQKg)\n",
        "\n",
        ">>[Prediction](#scrollTo=23ma5K3xSKrS)\n",
        "\n",
        ">>[More Details](#scrollTo=WIGdB_nfSK9o)\n",
        "\n",
        ">>[Compare results](#scrollTo=_6H6Ca-GSK4-)\n",
        "\n",
        ">[References](#scrollTo=tSbN2yDrRSdW)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1oBho425Q7ca",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1-Introduction"
      ]
    },
    {
      "metadata": {
        "id": "yLvtrsm0RcWq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* From [Francesco et al., 2011] **recommender systems** are software tools and techniques that provide **suggestions** for items to be of use of users.\n",
        "\n",
        "* The systems can suggest to the **user** things like what item to buy or what movie to watch.\n",
        "\n",
        "\n",
        "* Recommender system can predict reviews of users on new items, as well as they can predict items properties.\n",
        "\n",
        "* The **3 main**   approaches to build a Recommender System are: **Content Based** , **Collaborative filtering** and **Hybrid systems**.\n",
        "\n",
        "* An hybrid system is the one that combine different approaches in order to eliminate some disadvantages of these approaches"
      ]
    },
    {
      "metadata": {
        "id": "A8DvRRWlRdGi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Categories\n",
        "\n",
        "* In collaborative filtering, we can build a recommender system  following **2** other approaches: **Neighborhood ** approach and **model based** approach.\n",
        "* Each of the  approaches cited above, group a set of different techniques.\n",
        "\n",
        "* To summarize, the approaches to build a recommender system are:\n",
        "  * Collaborative Based Filtering: \n",
        "    * Neighborhood Approach\n",
        "      *  User-based approach\n",
        "      * Item-based approach\n",
        "    * Model Based Approach    \n",
        "      * SVM: Support Vector Machines\n",
        "      * SVD: Singular Value Decomposition\n",
        "  * Contend based Filtering: based on the items characteristics.\n",
        "  * Hybrid Systems\n",
        "        "
      ]
    },
    {
      "metadata": {
        "id": "OjzIaOe1RdZ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Libraries\n",
        "\n",
        "\n",
        "* There are different libraries and frameworks that we can use to build our recommender system:\n",
        "\n",
        "  * Surprise Library: [A Python scikit for recommender systems.](https://surpriselib.com/)\n",
        "\n",
        "  * Crab Library: [A Recommender System in python](http://muricoca.github.io/crab/install.html)\n",
        "\n",
        "\n",
        "  * Polylearn: [A library for factorization machines and polynomial networks](https://contrib.scikit-learn.org/polylearn/)\n",
        "\n",
        "  * Graphlab: [Simple development of custom machine learning models](https://turi.com/)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "fJHM9N6Whfhe",
        "colab_type": "code",
        "outputId": "bf869d6c-98c2-4a2b-e63a-78413e60badb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#installation of polynomial_network.pylylearn library (step 1)\n",
        "!git clone https://github.com/scikit-learn-contrib/polylearn.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'polylearn' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0bkxBA9NmbJY",
        "colab_type": "code",
        "outputId": "824af100-5b7f-4045-fb08-d27729e0bac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#installation of polylearn library (step 2 )\n",
        "\n",
        "!pip install sklearn-contrib-lightning"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn-contrib-lightning in /usr/local/lib/python3.6/dist-packages (0.5.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H_Ow6HYmhy_Q",
        "colab_type": "code",
        "outputId": "c218dd6e-26fb-4452-8e80-d90d077262a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2399
        }
      },
      "cell_type": "code",
      "source": [
        "#installation of polylearn library (step 3 )\n",
        "\n",
        "%cd polylearn\n",
        "!python setup.py build\n",
        "!sudo python setup.py install\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/polylearn/polylearn\n",
            "Appending polylearn.tests configuration to polylearn\n",
            "Ignoring attempt to set 'name' (from 'polylearn' to 'polylearn.tests')\n",
            "\u001b[39mrunning build\u001b[0m\n",
            "\u001b[39mrunning config_cc\u001b[0m\n",
            "\u001b[39munifing config_cc, config, build_clib, build_ext, build commands --compiler options\u001b[0m\n",
            "\u001b[39mrunning config_fc\u001b[0m\n",
            "\u001b[39munifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\u001b[0m\n",
            "\u001b[39mrunning build_src\u001b[0m\n",
            "\u001b[39mbuild_src\u001b[0m\n",
            "\u001b[39mbuilding extension \"polylearn.loss_fast\" sources\u001b[0m\n",
            "\u001b[39mbuilding extension \"polylearn.cd_direct_fast\" sources\u001b[0m\n",
            "\u001b[39mbuilding extension \"polylearn.cd_linear_fast\" sources\u001b[0m\n",
            "\u001b[39mbuilding extension \"polylearn.cd_lifted_fast\" sources\u001b[0m\n",
            "\u001b[39mbuild_src: building npy-pkg config files\u001b[0m\n",
            "\u001b[39mrunning build_py\u001b[0m\n",
            "\u001b[39mcreating build\u001b[0m\n",
            "\u001b[39mcreating build/lib.linux-x86_64-3.6\u001b[0m\n",
            "\u001b[39mcreating build/lib.linux-x86_64-3.6/polylearn\u001b[0m\n",
            "\u001b[39mcopying ./factorization_machine.py -> build/lib.linux-x86_64-3.6/polylearn\u001b[0m\n",
            "\u001b[39mcopying ./loss.py -> build/lib.linux-x86_64-3.6/polylearn\u001b[0m\n",
            "\u001b[39mcopying ./__init__.py -> build/lib.linux-x86_64-3.6/polylearn\u001b[0m\n",
            "\u001b[39mcopying ./polynomial_network.py -> build/lib.linux-x86_64-3.6/polylearn\u001b[0m\n",
            "\u001b[39mcopying ./base.py -> build/lib.linux-x86_64-3.6/polylearn\u001b[0m\n",
            "\u001b[39mcopying ./kernels.py -> build/lib.linux-x86_64-3.6/polylearn\u001b[0m\n",
            "\u001b[39mcreating build/lib.linux-x86_64-3.6/polylearn/tests\u001b[0m\n",
            "\u001b[39mcopying ./tests/test_cd_linear.py -> build/lib.linux-x86_64-3.6/polylearn/tests\u001b[0m\n",
            "\u001b[39mcopying ./tests/test_kernels.py -> build/lib.linux-x86_64-3.6/polylearn/tests\u001b[0m\n",
            "\u001b[39mcopying ./tests/test_common.py -> build/lib.linux-x86_64-3.6/polylearn/tests\u001b[0m\n",
            "\u001b[39mcopying ./tests/__init__.py -> build/lib.linux-x86_64-3.6/polylearn/tests\u001b[0m\n",
            "\u001b[39mcopying ./tests/test_polynomial_network.py -> build/lib.linux-x86_64-3.6/polylearn/tests\u001b[0m\n",
            "\u001b[39mcopying ./tests/test_factorization_machine.py -> build/lib.linux-x86_64-3.6/polylearn/tests\u001b[0m\n",
            "\u001b[39mrunning build_ext\u001b[0m\n",
            "\u001b[39mcustomize UnixCCompiler\u001b[0m\n",
            "\u001b[39mcustomize UnixCCompiler using build_ext\u001b[0m\n",
            "\u001b[39mcustomize UnixCCompiler\u001b[0m\n",
            "\u001b[39mcustomize UnixCCompiler using build_ext\u001b[0m\n",
            "\u001b[39mbuilding 'polylearn.loss_fast' extension\u001b[0m\n",
            "\u001b[39mcompiling C++ sources\u001b[0m\n",
            "\u001b[39mC compiler: x86_64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
            "\u001b[0m\n",
            "\u001b[39mcompile options: '-I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c'\u001b[0m\n",
            "\u001b[39mx86_64-linux-gnu-g++: ./loss_fast.cpp\u001b[0m\n",
            "\u001b[39mx86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/loss_fast.o -o build/lib.linux-x86_64-3.6/polylearn/loss_fast.cpython-36m-x86_64-linux-gnu.so\u001b[0m\n",
            "\u001b[39mbuilding 'polylearn.cd_direct_fast' extension\u001b[0m\n",
            "\u001b[39mcompiling C++ sources\u001b[0m\n",
            "\u001b[39mC compiler: x86_64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
            "\u001b[0m\n",
            "\u001b[39mcompile options: '-I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c'\u001b[0m\n",
            "\u001b[39mx86_64-linux-gnu-g++: ./cd_direct_fast.cpp\u001b[0m\n",
            "./cd_direct_fast.cpp: In function ‘PyObject* __pyx_pw_9polylearn_14cd_direct_fast_1_cd_direct_ho(PyObject*, PyObject*, PyObject*)’:\n",
            "./cd_direct_fast.cpp:18439:34: warning: ‘__pyx_v_it’ may be used uninitialized in this function [-Wmaybe-uninitialized]\n",
            "             return PyInt_FromLong((long) value);\n",
            "                                  ^\n",
            "./cd_direct_fast.cpp:2701:16: note: ‘__pyx_v_it’ was declared here\n",
            "   unsigned int __pyx_v_it;\n",
            "                ^~~~~~~~~~\n",
            "\u001b[39mx86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cd_direct_fast.o -o build/lib.linux-x86_64-3.6/polylearn/cd_direct_fast.cpython-36m-x86_64-linux-gnu.so\u001b[0m\n",
            "\u001b[39mbuilding 'polylearn.cd_linear_fast' extension\u001b[0m\n",
            "\u001b[39mcompiling C++ sources\u001b[0m\n",
            "\u001b[39mC compiler: x86_64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
            "\u001b[0m\n",
            "\u001b[39mcompile options: '-I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c'\u001b[0m\n",
            "\u001b[39mx86_64-linux-gnu-g++: ./cd_linear_fast.cpp\u001b[0m\n",
            "\u001b[39mx86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cd_linear_fast.o -o build/lib.linux-x86_64-3.6/polylearn/cd_linear_fast.cpython-36m-x86_64-linux-gnu.so\u001b[0m\n",
            "\u001b[39mbuilding 'polylearn.cd_lifted_fast' extension\u001b[0m\n",
            "\u001b[39mcompiling C++ sources\u001b[0m\n",
            "\u001b[39mC compiler: x86_64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC\n",
            "\u001b[0m\n",
            "\u001b[39mcompile options: '-I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c'\u001b[0m\n",
            "\u001b[39mx86_64-linux-gnu-g++: ./cd_lifted_fast.cpp\u001b[0m\n",
            "./cd_lifted_fast.cpp: In function ‘PyObject* __pyx_pw_9polylearn_14cd_lifted_fast_3_cd_lifted(PyObject*, PyObject*, PyObject*)’:\n",
            "./cd_lifted_fast.cpp:18546:34: warning: ‘__pyx_v_it’ may be used uninitialized in this function [-Wmaybe-uninitialized]\n",
            "             return PyInt_FromLong((long) value);\n",
            "                                  ^\n",
            "./cd_lifted_fast.cpp:2621:7: note: ‘__pyx_v_it’ was declared here\n",
            "   int __pyx_v_it;\n",
            "       ^~~~~~~~~~\n",
            "\u001b[39mx86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cd_lifted_fast.o -o build/lib.linux-x86_64-3.6/polylearn/cd_lifted_fast.cpython-36m-x86_64-linux-gnu.so\u001b[0m\n",
            "Warning: Assuming default configuration (./tests/{setup_tests,setup}.py was not found)Appending polylearn.tests configuration to polylearn\n",
            "Ignoring attempt to set 'name' (from 'polylearn' to 'polylearn.tests')\n",
            "\u001b[39mrunning install\u001b[0m\n",
            "\u001b[39mrunning build\u001b[0m\n",
            "\u001b[39mrunning config_cc\u001b[0m\n",
            "\u001b[39munifing config_cc, config, build_clib, build_ext, build commands --compiler options\u001b[0m\n",
            "\u001b[39mrunning config_fc\u001b[0m\n",
            "\u001b[39munifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\u001b[0m\n",
            "\u001b[39mrunning build_src\u001b[0m\n",
            "\u001b[39mbuild_src\u001b[0m\n",
            "\u001b[39mbuilding extension \"polylearn.loss_fast\" sources\u001b[0m\n",
            "\u001b[39mbuilding extension \"polylearn.cd_direct_fast\" sources\u001b[0m\n",
            "\u001b[39mbuilding extension \"polylearn.cd_linear_fast\" sources\u001b[0m\n",
            "\u001b[39mbuilding extension \"polylearn.cd_lifted_fast\" sources\u001b[0m\n",
            "\u001b[39mbuild_src: building npy-pkg config files\u001b[0m\n",
            "\u001b[39mrunning build_py\u001b[0m\n",
            "\u001b[39mrunning build_ext\u001b[0m\n",
            "\u001b[39mcustomize UnixCCompiler\u001b[0m\n",
            "\u001b[39mcustomize UnixCCompiler using build_ext\u001b[0m\n",
            "\u001b[39mcustomize UnixCCompiler\u001b[0m\n",
            "\u001b[39mcustomize UnixCCompiler using build_ext\u001b[0m\n",
            "\u001b[39mrunning install_lib\u001b[0m\n",
            "\u001b[39mcreating /usr/local/lib/python3.6/dist-packages/polylearn\u001b[0m\n",
            "\u001b[39mcreating /usr/local/lib/python3.6/dist-packages/polylearn/tests\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/tests/test_cd_linear.py -> /usr/local/lib/python3.6/dist-packages/polylearn/tests\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/tests/test_kernels.py -> /usr/local/lib/python3.6/dist-packages/polylearn/tests\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/tests/test_common.py -> /usr/local/lib/python3.6/dist-packages/polylearn/tests\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/tests/__init__.py -> /usr/local/lib/python3.6/dist-packages/polylearn/tests\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/tests/test_polynomial_network.py -> /usr/local/lib/python3.6/dist-packages/polylearn/tests\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/tests/test_factorization_machine.py -> /usr/local/lib/python3.6/dist-packages/polylearn/tests\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/cd_lifted_fast.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages/polylearn\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/cd_linear_fast.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages/polylearn\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/factorization_machine.py -> /usr/local/lib/python3.6/dist-packages/polylearn\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/loss.py -> /usr/local/lib/python3.6/dist-packages/polylearn\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/__init__.py -> /usr/local/lib/python3.6/dist-packages/polylearn\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/polynomial_network.py -> /usr/local/lib/python3.6/dist-packages/polylearn\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/cd_direct_fast.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages/polylearn\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/base.py -> /usr/local/lib/python3.6/dist-packages/polylearn\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/loss_fast.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages/polylearn\u001b[0m\n",
            "\u001b[39mcopying build/lib.linux-x86_64-3.6/polylearn/kernels.py -> /usr/local/lib/python3.6/dist-packages/polylearn\u001b[0m\n",
            "\u001b[39mbyte-compiling /usr/local/lib/python3.6/dist-packages/polylearn/tests/test_cd_linear.py to test_cd_linear.cpython-36.pyc\u001b[0m\n",
            "\u001b[39mbyte-compiling /usr/local/lib/python3.6/dist-packages/polylearn/tests/test_kernels.py to test_kernels.cpython-36.pyc\u001b[0m\n",
            "\u001b[39mbyte-compiling /usr/local/lib/python3.6/dist-packages/polylearn/tests/test_common.py to test_common.cpython-36.pyc\u001b[0m\n",
            "\u001b[39mbyte-compiling /usr/local/lib/python3.6/dist-packages/polylearn/tests/__init__.py to __init__.cpython-36.pyc\u001b[0m\n",
            "\u001b[39mbyte-compiling /usr/local/lib/python3.6/dist-packages/polylearn/tests/test_polynomial_network.py to test_polynomial_network.cpython-36.pyc\u001b[0m\n",
            "\u001b[39mbyte-compiling /usr/local/lib/python3.6/dist-packages/polylearn/tests/test_factorization_machine.py to test_factorization_machine.cpython-36.pyc\u001b[0m\n",
            "\u001b[39mbyte-compiling /usr/local/lib/python3.6/dist-packages/polylearn/factorization_machine.py to factorization_machine.cpython-36.pyc\u001b[0m\n",
            "\u001b[39mbyte-compiling /usr/local/lib/python3.6/dist-packages/polylearn/loss.py to loss.cpython-36.pyc\u001b[0m\n",
            "\u001b[39mbyte-compiling /usr/local/lib/python3.6/dist-packages/polylearn/__init__.py to __init__.cpython-36.pyc\u001b[0m\n",
            "\u001b[39mbyte-compiling /usr/local/lib/python3.6/dist-packages/polylearn/polynomial_network.py to polynomial_network.cpython-36.pyc\u001b[0m\n",
            "\u001b[39mbyte-compiling /usr/local/lib/python3.6/dist-packages/polylearn/base.py to base.cpython-36.pyc\u001b[0m\n",
            "\u001b[39mbyte-compiling /usr/local/lib/python3.6/dist-packages/polylearn/kernels.py to kernels.cpython-36.pyc\u001b[0m\n",
            "\u001b[39mrunning install_egg_info\u001b[0m\n",
            "\u001b[39mWriting /usr/local/lib/python3.6/dist-packages/polylearn-0.0.0.egg-info\u001b[0m\n",
            "\u001b[39mrunning install_clib\u001b[0m\n",
            "\u001b[39mcustomize UnixCCompiler\u001b[0m\n",
            "Warning: Assuming default configuration (./tests/{setup_tests,setup}.py was not found)"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VL49qQMiRCfC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2- Collaborative filtering"
      ]
    },
    {
      "metadata": {
        "id": "QtI6YjSJRzo8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neighborhood approach\n",
        "\n",
        "\n",
        "* In collaborative filtering, to predict ratings of a** user u** on an **item j**,  the ratings of that user are taken into account, as well as the ratings of **other users**.\n",
        "\n",
        "* In **neighborhood approach**(**memory-based**), the ratings are used directly to make these predictions following** 2 **approaches:\n",
        "\n",
        "  * **User-based** recommendation: the ratings of a user **u** for an item** j** are obtained from the ratings given by  the** neighbors** of that user to that item. The **neighbors **are those who had **similar rating** patterns as the user** u**  for other items that they have rated in **common**. \n",
        "\n",
        "  * **item-based** recommendation: the ratings of a user **u** for an item**j ** are obtained from the ratings of that user **u** for other **similar items ** to the item** j**. Two items are **similar**, if they have been rated in the **same way** by** other users**.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Tg2exEipRz7w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model based approach\n",
        "\n",
        "\n",
        "* In this case, the ratings of users for items are not used directly. They are instead used to create a model.\n",
        "\n",
        "* **The created model** will be used later to predict **ratings** for **new items**.\n",
        "\n",
        "  * Some of them are **latent factor models**. They rely on the idea that there are **latent** (**hidden**) **characteristics** of the users and items. So, the interaction user-item will be **modeled** with **factors** that represent these characteristics.\n",
        "\n",
        "    * Several techniques can be used as **matrix factorization** with **singular value decomposition**.\n",
        "  * The models can also be created using supervised learning techniques. They are trained using the user-item iteractions. And then used to predict new values.\n",
        "In this case, Support vector machines (SVM) can be used. "
      ]
    },
    {
      "metadata": {
        "id": "Zd06of0ZR7JC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Knn algorithm\n",
        "\n",
        "* **K nearest neighborhood** algorithm can be used for both **item-based** and **user-based** recommendation.\n",
        "\n",
        "* The following steps, describe the application of this algorithm for **user-based** recommendation, to predict ratings of user **u** on item** j**:\n",
        "\n",
        "  * Each user is represented by a **vector** of his ** ratings**\n",
        "\n",
        "  * A **similarity measure** is selected to identify **similar users**.\n",
        "\n",
        "* Find the**k most similar users** to the user **u**, that have rated the **item j**.\n",
        "\n",
        "* Predict  the **ratings** of the user **u** for the item** j** by computing the **weighted average** of the ratings of the **k neighbors** of the user **u**, for the item** j**.\n"
      ]
    },
    {
      "metadata": {
        "id": "Nhm8OJc_RFU8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3- Similarity scores\n"
      ]
    },
    {
      "metadata": {
        "id": "TOgMnI_CR7QE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "* A** similarity score** is a value that describes the **degree** of **similarity** between **users** or **items**.\n",
        "\n",
        "* This degree can be computed using **different formula**:\n",
        "\n",
        "  * ** Cosine similarity** : the users (items) are** vectors**, and the similarity is described by the **cosine **of the **angle** formed by a pair of these vectors.\n",
        "\n",
        "  * **MSD**: the **mean squared difference **between pairs of users (items)\n",
        "\n",
        "  * ** Pearson score**: measures the **correlation** (linear relationship) between pairs of items (users).\n",
        "\n",
        "  * **Pearson score** with a **shrinkage** parameter:  computes the correlation between pairs using **baselines** and a shrunk parameter , to avoid overfitting.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iJCObGCDR7Wk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cosine Similarity\n",
        "\n",
        "* The formula is as follow:\n",
        "$\\text{cosine_sim}(u, v) = \\frac{\n",
        "\\sum\\limits_{i \\in I_{uv}} r_{ui} \\cdot r_{vi}}\n",
        "{\\sqrt{\\sum\\limits_{i \\in I_{uv}} r_{ui}^2} \\cdot\n",
        "\\sqrt{\\sum\\limits_{i \\in I_{uv}} r_{vi}^2}\n",
        "}$\n",
        "> \n",
        "\n",
        "    $\\text{cosine_sim}(i, j) = \\frac{\n",
        "\\sum\\limits_{u \\in U_{ij}} r_{ui} \\cdot r_{uj}}\n",
        "{\\sqrt{\\sum\\limits_{u \\in U_{ij}} r_{ui}^2} \\cdot\n",
        "\\sqrt{\\sum\\limits_{u \\in U_{ij}} r_{uj}^2}\n",
        "}$\n",
        "\n",
        "  >  where : $r_{ui}$ is the rating of the user $u$ for the item $i$.\n",
        "  \n",
        "  > $I_{uv}$ are **items** in **common** between the users $u$ and $v$. And $U_{ij}$ are **users** in common between **users** $u$ and $v$\n",
        "  \n",
        "* Only the common users (items) are taken into account.  The formula is used either to compute the similarity score between **users** or between **items**. The values range from **0 ** to **1**  "
      ]
    },
    {
      "metadata": {
        "id": "qFh5dIXNRbde",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pearson similarity\n",
        "\n",
        "* It is computed as follow:\n",
        "\n",
        "  >\n",
        "  $\\text{pearson_sim}(u, v) = \\frac{ \\sum\\limits_{i \\in I_{uv}}\n",
        "(r_{ui} -  \\mu_u) \\cdot (r_{vi} - \\mu_{v})} {\\sqrt{\\sum\\limits_{i\n",
        "\\in I_{uv}} (r_{ui} -  \\mu_u)^2} \\cdot \\sqrt{\\sum\\limits_{i \\in\n",
        "I_{uv}} (r_{vi} -  \\mu_{v})^2} }$\n",
        "\n",
        "\n",
        "> $\\text{pearson_sim}(i, j) = \\frac{ \\sum\\limits_{u \\in U_{ij}}\n",
        "(r_{ui} -  \\mu_i) \\cdot (r_{uj} - \\mu_{j})} {\\sqrt{\\sum\\limits_{u\n",
        "\\in U_{ij}} (r_{ui} -  \\mu_i)^2} \\cdot \\sqrt{\\sum\\limits_{u \\in\n",
        "U_{ij}} (r_{uj} -  \\mu_{j})^2} }$\n",
        "\n",
        "   > where: $\\mu_{i}$  is the mean of the ratings of the **item i**.\n",
        "   > And $\\mu_u$ is the mean of the ratings made by the **user u**\n",
        "   \n",
        "* Again, only the common user (items)   are taken into account. This score can be seen as the **mean centered cosine ** similarity score. The values range from **-1** to **1**."
      ]
    },
    {
      "metadata": {
        "id": "Ca8Ofrs7RHwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4- Cross-Validation"
      ]
    },
    {
      "metadata": {
        "id": "xn8bZSmqSBwY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "* To measure the performance of a  model, we split the data into **training** and **testing** set. We train the data using only the training set. And, finally we test our model on the testing sets.\n",
        "\n",
        "* But, when we want to identify the best parameters  for ou  estimators, we train and **test** our models several times. **Indirectly**, the test set values will interfere in the training. And the metrics used to estimate our models, will no longer reflect the actual performance of the model.\n",
        "\n",
        "* To avoid this situation, another split is necessary: the **validation set**. We use it to test our parameters. And when we are done, we perform a final test on the test data.\n",
        "\n",
        "* But when the data is too small, we use instead a **cross-validation** technique without using a validation set."
      ]
    },
    {
      "metadata": {
        "id": "157T7zEfSB_u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## K-fold Cross validation\n",
        "\n",
        "* The cross-validation technique can be applied with different approaches. A basic one is the** k-fold cross-validation**\n",
        "\n",
        "  * The data set is split into** k** smaller **folds**\n",
        "  * The model is trained on the **k-1 folds**\n",
        "  * For each training, the model is tested on the **remaining fold.**  And **a corresponding** score is computed.\n",
        "  * **The performance  score**   of the model is the average of all the scores computed previously\n",
        "\n",
        "* The cross validation technique is generally combined with an other tool: the **Grid Search** tool( already seen in **Ensemble Learning Lessons**)\n",
        "\n",
        "* In fact, the GridSerchCVclass of scklearn can be parameterized by specifying the  **cv** parameter (the cross validations strategy to use)."
      ]
    },
    {
      "metadata": {
        "id": "ArfuFPFFSB4a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cross-validation implementation\n",
        "\n",
        "* In **sklearn**, we can use the **cross-validation** in different manners:\n",
        "\n",
        "  * Using the cross validation indirectly by using the  **GridSearchCV** class\n",
        "\n",
        "  * Using the **cross_validate** and **cross_val_score** functions\n",
        "\n",
        "  * Splitting the data using the corresponding fold strategy for a cross-validation approach as the** KFold** class\n",
        "\n",
        "* In *Surprise* library, that we will use for our Recommender Systems examples, implements the cross-validation as well:\n",
        "\n",
        "  * Using the **GridSearchCV** class and iterators as **Kfold**\n",
        "\n",
        "  * Using the **cross_validate** function"
      ]
    },
    {
      "metadata": {
        "id": "5aXVjrUrRNGi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5- User-based Collaborative Filtering With Surprise"
      ]
    },
    {
      "metadata": {
        "id": "H71qigLHSGU2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Library and data"
      ]
    },
    {
      "metadata": {
        "id": "N-lmUZW6qpU-",
        "colab_type": "code",
        "outputId": "01b11acc-f889-475b-ad90-764505816fb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "#installation of surprise library\n",
        "!pip install surprise"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: surprise in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.6/dist-packages (from surprise) (1.0.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (0.13.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.14.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-dmEPJzNtoD4",
        "colab_type": "code",
        "outputId": "2298e6c2-f9bb-422e-ac41-818e7690a874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "from surprise import Dataset\n",
        "\n",
        "# load movielens-100k dataset\n",
        "myData = Dataset.load_builtin('ml-100k')\n",
        "\n",
        "# Retrieve the trainset.\n",
        "trainset = myData.build_full_trainset()\n",
        "\n",
        "print (\"The number of items = \",trainset.n_items)\n",
        "print (\"The number of users = \",trainset.n_users)\n",
        "print (\"The number of ratings= \",trainset.n_ratings)\n",
        "# the items and users IDs are inner IDs\n",
        "# those created by the build_full_trainset\n",
        "print(\"The rating for the item 0 by the user \",trainset.ir[0][2][0],\" is:\",trainset.ir[0][2][1])\n",
        "print(\"The rating of the user 0 for the item\",trainset.ur[0][1][0],\" is:\",trainset.ur[0][1][1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of items =  1682\n",
            "The number of users =  943\n",
            "The number of ratings=  100000\n",
            "The rating for the item 0 by the user  218  is: 5.0\n",
            "The rating of the user 0 for the item 528  is: 4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yMgLDZAdSGjG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Similarity matrix computation (fitting)"
      ]
    },
    {
      "metadata": {
        "id": "SjGTcfc7SJto",
        "colab_type": "code",
        "outputId": "ef43db33-3187-4a6b-e6e6-50af1df3a83a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "cell_type": "code",
      "source": [
        "from surprise import KNNBasic\n",
        "\n",
        "\n",
        "# We'll use the basic Knn algorithm.\n",
        "# we will use a user based estimator, using cosine score\n",
        "Recommender = KNNBasic(sim_options={\"name\":\"cosine\",\"user_based\":True})\n",
        "\n",
        "# fitting to the data ==> compute similarity scores between users\n",
        "Recommender.fit(trainset)\n",
        "#the similarity matrix\n",
        "Recommender.sim\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.87278605, 0.91226401, ..., 0.86717176, 0.84366149,\n",
              "        0.9486833 ],\n",
              "       [0.87278605, 1.        , 0.84761034, ..., 0.8782826 , 0.87552384,\n",
              "        0.94252177],\n",
              "       [0.91226401, 0.84761034, 1.        , ..., 0.88184244, 1.        ,\n",
              "        0.90116647],\n",
              "       ...,\n",
              "       [0.86717176, 0.8782826 , 0.88184244, ..., 1.        , 0.89504128,\n",
              "        0.93603858],\n",
              "       [0.84366149, 0.87552384, 1.        , ..., 0.89504128, 1.        ,\n",
              "        0.98994949],\n",
              "       [0.9486833 , 0.94252177, 0.90116647, ..., 0.93603858, 0.98994949,\n",
              "        1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "Z7prBx0pSGbS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prediction\n",
        "* The used formulas:\n",
        "  * user-based estimation:\n",
        "> $\\hat{r}_{ui} = \\frac{\n",
        "\\sum\\limits_{v \\in N^k_i(u)} \\text{sim}(u, v) \\cdot r_{vi}}\n",
        "{\\sum\\limits_{v \\in N^k_i(u)} \\text{sim}(u, v)}$\n",
        "  * item-base estimation:\n",
        "  \n",
        "    > $\\hat{r}_{ui} = \\frac{\n",
        "\\sum\\limits_{j \\in N^k_u(i)} \\text{sim}(i, j) \\cdot r_{uj}}\n",
        "{\\sum\\limits_{j \\in N^k_u(j)} \\text{sim}(i, j)}$"
      ]
    },
    {
      "metadata": {
        "id": "oBuCiGCelgKK",
        "colab_type": "code",
        "outputId": "492dce5b-1aaf-4527-f827-cd1c14ce6d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "user_raw_id = trainset.to_raw_uid(218)\n",
        "item_raw_id=  trainset.to_raw_iid(528)\n",
        "print(Recommender.predict(user_raw_id,item_raw_id))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "user: 226        item: 393        r_ui = None   est = 3.58   {'actual_k': 40, 'was_impossible': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CffbTqAERQKg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6- Item-based Collaborative Filtering With Surprise"
      ]
    },
    {
      "metadata": {
        "id": "23ma5K3xSKrS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ]
    },
    {
      "metadata": {
        "id": "tdxsy-gExcyA",
        "colab_type": "code",
        "outputId": "3f24a856-10f7-4df0-eb4a-5d25a4def19a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "# Now, we will use an item based estimator, using cosine score\n",
        "Recommender2 = KNNBasic(sim_options={\"name\":\"cosine\",\"user_based\":False})\n",
        "\n",
        "# fitting to the data ==> compute similarity scores between items\n",
        "Recommender2.fit(trainset)\n",
        "#the similarity matrix\n",
        "print( \"size of similarity matrix of User-based Recommender: \", Recommender.sim.shape)\n",
        "print( \"size of similarity matrix of Item-based Recommender: \", Recommender2.sim.shape)\n",
        "\n",
        "print(Recommender2.predict(user_raw_id,item_raw_id))\n",
        "# in surprise they take into account only positive similarities\n",
        "# which is not the case in our example. \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "size of similarity matrix of User-based Recommender:  (943, 943)\n",
            "size of similarity matrix of Item-based Recommender:  (1682, 1682)\n",
            "user: 226        item: 393        r_ui = None   est = 3.85   {'actual_k': 40, 'was_impossible': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WIGdB_nfSK9o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## More Details"
      ]
    },
    {
      "metadata": {
        "id": "znMJjCyX1E3G",
        "colab_type": "code",
        "outputId": "40791361-ff0d-4a20-c610-7096ec2687c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#number of neighbors\n",
        "k= Recommender2.k\n",
        "print(\"Number of neighbors taken into account:\",k)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of neighbors taken into account: 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BnokSTbXu2t6",
        "colab_type": "code",
        "outputId": "2e300a2d-d644-41ad-a161-a37ce60b8187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# RL number of items rated by user 218 (rawid =\"226\")\n",
        "RL = len(trainset.ur[218])\n",
        "\n",
        "# inner ids of items rates by the user 218\n",
        "ratedByU =[trainset.ur[218][i][0] for i in range(RL)]\n",
        "# the corresponding ratings by the user 218\n",
        "ratesOfU =[trainset.ur[218][i][1] for i in range(RL)] \n",
        "# similarities between the item 528 (rawid =\"393\")\n",
        "similaritiesU = Recommender2.sim[528,ratedByU]\n",
        "# the indices of ordred similarites (descending order)\n",
        "# it was ascending, then [::-1]reversed the order\n",
        "indSortU = np.argsort(similaritiesU)[::-1]\n",
        "\n",
        "# select the k indices corresponding to\n",
        "# the k greatest sorted similarities \n",
        "if RL < k :\n",
        "  indSortUk = indSortU[:RL]  \n",
        "else:\n",
        "  indSortUk = indSortU[:k]\n",
        "\n",
        " # select the k sorted greatest similarities \n",
        "similaritiesUk = similaritiesU[indSortUk]\n",
        "ratesOfU = np.asarray(ratesOfU)\n",
        "# selct the k corresponding ratings\n",
        "ratesOfUk = ratesOfU[indSortUk]\n",
        "# the sum of the k similarities\n",
        "simSum = similaritiesUk.sum()\n",
        "# the sum of the weighted similarities\n",
        "# the weights are the corresponding ratings\n",
        "weightedSum = (similaritiesUk * ratesOfUk).sum()\n",
        "# the estimated rating\n",
        "finalRating = weightedSum/simSum\n",
        "print(\"Estimated rating of the user 218 ('226'), for the item 528('393') is : %1.2f\" % finalRating)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimated rating of the user 218 ('226'), for the item 528('393') is : 3.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_6H6Ca-GSK4-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compare results\n",
        "\n",
        "* We will use the ```cross-validate``` function from ``` Surprise.model_selection``` package in order to compare the performances of the **user** and** item **based **Recommender Systems**. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VPotjtGj24N4",
        "colab_type": "code",
        "outputId": "59050aba-fa25-4712-92c8-5d780de7df68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "cell_type": "code",
      "source": [
        "from surprise.model_selection import cross_validate\n",
        "cross_validate(Recommender, myData, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Evaluating RMSE, MAE of algorithm KNNBasic on 5 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
            "RMSE (testset)    1.0322  1.0101  1.0113  1.0131  1.0189  1.0171  0.0081  \n",
            "MAE (testset)     0.8164  0.7983  0.7985  0.7999  0.8083  0.8043  0.0071  \n",
            "Fit time          1.09    1.13    1.12    1.10    1.09    1.10    0.02    \n",
            "Test time         3.92    4.04    4.00    4.01    3.91    3.98    0.05    \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': (1.0948317050933838,\n",
              "  1.1269288063049316,\n",
              "  1.1188697814941406,\n",
              "  1.0966784954071045,\n",
              "  1.0857350826263428),\n",
              " 'test_mae': array([0.81637912, 0.79826172, 0.79848197, 0.79994099, 0.80829388]),\n",
              " 'test_rmse': array([1.03215323, 1.01014902, 1.01131544, 1.01307439, 1.0188665 ]),\n",
              " 'test_time': (3.9210283756256104,\n",
              "  4.039148330688477,\n",
              "  3.998549699783325,\n",
              "  4.010190010070801,\n",
              "  3.9080941677093506)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "metadata": {
        "id": "ZOZ5T8Xi3BqI",
        "colab_type": "code",
        "outputId": "9799487c-c081-463b-c235-52738a690b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "cell_type": "code",
      "source": [
        "cross_validate(Recommender2, myData, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Evaluating RMSE, MAE of algorithm KNNBasic on 5 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
            "RMSE (testset)    1.0326  1.0338  1.0269  1.0278  1.0159  1.0274  0.0063  \n",
            "MAE (testset)     0.8170  0.8191  0.8111  0.8104  0.8053  0.8126  0.0050  \n",
            "Fit time          1.86    1.88    1.97    1.76    1.75    1.85    0.08    \n",
            "Test time         4.83    4.86    4.73    4.52    4.61    4.71    0.13    \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': (1.8648626804351807,\n",
              "  1.8790833950042725,\n",
              "  1.9736132621765137,\n",
              "  1.7609879970550537,\n",
              "  1.749342441558838),\n",
              " 'test_mae': array([0.81703881, 0.81910724, 0.81114783, 0.81042389, 0.80525179]),\n",
              " 'test_rmse': array([1.03263114, 1.03379201, 1.02687778, 1.02775705, 1.01588415]),\n",
              " 'test_time': (4.830214738845825,\n",
              "  4.860896110534668,\n",
              "  4.72918438911438,\n",
              "  4.524571657180786,\n",
              "  4.614382266998291)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "metadata": {
        "id": "tSbN2yDrRSdW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* [Francesco et al., 2011] Francesco, R., Lior, R., Bracha, S., and Paul B., K., editors (2011). Recommender Systems Handbook. Springer Science+Business Media.\n",
        "\n",
        "* [Grčar et al., 2006] Grčar, M., Fortuna, B., Mladenič, D., and Grobelnik, M. (2006). knn versus svm in the collaborative filtering framework. In Data Science and Classification, pages 251–260. Springer.\n",
        "\n",
        "* [Hug, 2017] Hug, N. (2017). Surprise, a Python library for recommender systems. http://surpriselib.com.\n",
        "\n",
        "* [kumar Bokde et al., 2015] kumar Bokde, D., Girase, S., and Mukhopadhyay,D. (2015). Role of matrix factorization model in collaborative filtering algorithm: A survey. CoRR,abs/1503.07475."
      ]
    }
  ]
}